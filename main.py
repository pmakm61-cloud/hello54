# main.py
#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–∞—Ä—Å–µ—Ä–∞ hello54.ru
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from dotenv import load_dotenv
from src.database import DatabaseManager
from src.crawler import Hello54Crawler

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º .env –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
BASE_DIR = Path(__file__).resolve().parent
load_dotenv(BASE_DIR / '.env')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞—Ä–æ–ª—è
if not os.getenv('DB_PASSWORD'):
    print("‚ùå –û–®–ò–ë–ö–ê: .env —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ DB_PASSWORD –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    print(f"   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª: {BASE_DIR / '.env'}")
    print("   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ:")
    print("   DB_HOST=localhost")
    print("   DB_PORT=5432")
    print("   DB_NAME=hello54_parser")
    print("   DB_USER=postgres")
    print("   DB_PASSWORD=–≤–∞—à_–ø–∞—Ä–æ–ª—å")
    exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data/logs/parser.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ hello54.ru')
    parser.add_argument('--category', type=str, help='URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
    parser.add_argument('--categories-file', type=str, help='–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π')
    parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É')
    parser.add_argument('--export', type=str, help='–≠–∫—Å–ø–æ—Ä—Ç URL –≤ —Ñ–∞–π–ª (csv –∏–ª–∏ txt)')
    parser.add_argument('--max-pages', type=int, default=5,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    
    args = parser.parse_args()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    db = DatabaseManager()
    
    if args.stats:
        # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = db.get_statistics()
        if stats:
            print("\n" + "="*60)
            print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ï–†–ê")
            print("="*60)
            print(f"üìä –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {stats['stats']['total_products']}")
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['stats']['parsed_products']}")
            print(f"üìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {stats['stats']['total_categories']}")
            print(f"üïí –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {stats['stats']['last_update']}")
            
            print(f"\nüìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for cat in stats['categories']:
                print(f"  ‚Ä¢ {cat['name'] or cat['url']}: {cat['product_count']} —Ç–æ–≤–∞—Ä–æ–≤")
        
        db.close()
        return
    
    if args.category:
        # –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        crawler = Hello54Crawler(db)
        urls = crawler.parse_category(args.category, max_pages_override=args.max_pages)
        
        print(f"\n" + "="*50)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê")
        print("="*50)
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {args.category}")
        print(f"   –ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü: {args.max_pages}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(urls)}")
        
        if urls:
            print(f"\nüîó –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL:")
            for i, url in enumerate(urls[:3], 1):
                print(f"   {i}. {url}")
            if len(urls) > 3:
                print(f"   ... –∏ –µ—â—ë {len(urls) - 3} URL")
        
        if args.export:
            export_urls(urls, args.export)
    
    elif args.categories_file:
        # –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ —Ñ–∞–π–ª–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        try:
            with open(args.categories_file, 'r', encoding='utf-8') as f:
                categories = [line.strip() for line in f if line.strip()]
            
            print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            
            crawler = Hello54Crawler(db)
            
            for i, category_url in enumerate(categories, 1):
                print(f"\n[{i}/{len(categories)}] –ü–∞—Ä—Å–∏–Ω–≥: {category_url}")
                urls = crawler.parse_category(category_url, max_pages_override=args.max_pages)
                print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
    
    else:
        parser.print_help()
    
    db.close()

def export_urls(urls, filename):
    """–≠–∫—Å–ø–æ—Ä—Ç URL –≤ —Ñ–∞–π–ª"""
    try:
        if filename.endswith('.csv'):
            import pandas as pd
            df = pd.DataFrame({'url': urls})
            df.to_csv(filename, index=False, encoding='utf-8-sig')
        else:
            with open(filename, 'w', encoding='utf-8') as f:
                for url in urls:
                    f.write(url + '\n')
        
        print(f"üíæ URL —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")

if __name__ == "__main__":
    main()