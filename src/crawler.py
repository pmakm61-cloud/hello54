# src/crawler.py
import requests
from bs4 import BeautifulSoup
import time
import re
import urllib.parse
import logging
from datetime import datetime
from src.config import PARSER_CONFIG

logger = logging.getLogger(__name__)

class Hello54Crawler:
    """–ü–∞—Ä—Å–µ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–∞–π—Ç–∞ hello54.ru"""
    
    def __init__(self, db_manager):
        self.base_url = "https://hello54.ru"
        self.db = db_manager
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': PARSER_CONFIG['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        })
    
    def parse_category(self, category_url, max_pages_override=None):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 5 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)"""
        start_time = datetime.now()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        max_pages = max_pages_override or PARSER_CONFIG['max_pages_per_category']
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
        logger.info(f"‚öôÔ∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –º–∞–∫—Å–∏–º—É–º {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ –ë–î
        category_name = self._extract_category_name(category_url)
        category_id = self.db.save_category(category_url, category_name)
        
        if not category_id:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é")
            return []
        
        all_product_urls = []
        page_num = 1
        
        try:
            while page_num <= max_pages:  # ‚Üê –ó–¥–µ—Å—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
                page_url = self._get_page_url(category_url, page_num)
                logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{max_pages}: {page_url}")
                
                page_html = self._fetch_page(page_url)
                if not page_html:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}")
                    break
                
                page_urls = self._extract_product_urls(page_html, category_url)
                logger.info(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(page_urls)}")
                
                added = self.db.save_product_urls(page_urls, category_id)
                all_product_urls.extend(page_urls)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                self.show_progress(page_num, max_pages, len(all_product_urls))
                
                # –û–°–¢–ê–ù–û–í–ò–¢–¨ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü
                if page_num >= max_pages:
                    logger.info(f"\n‚èπÔ∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                if not self._has_next_page(page_html, page_num):
                    logger.info(f"\n‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                    break
                
                page_num += 1
                time.sleep(PARSER_CONFIG['delay_between_requests'])
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        duration = (datetime.now() - start_time).total_seconds()
        self.db.log_parse_session(
            category_url=category_url,
            action="category_parse",
            details=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {page_num-1} —Å—Ç—Ä–∞–Ω–∏—Ü (–ª–∏–º–∏—Ç {max_pages})",
            products_found=len(all_product_urls),
            products_added=len(set(all_product_urls)),
            duration=duration
        )
        
        unique_urls = list(set(all_product_urls))
        logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –ù–∞–π–¥–µ–Ω–æ {len(unique_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        logger.info(f"   –ü–æ—Ç—Ä–∞—á–µ–Ω–æ: {duration:.1f} —Å–µ–∫, {len(unique_urls)/max(duration, 0.1):.1f} —Ç–æ–≤–∞—Ä–æ–≤/—Å–µ–∫")
        
        return unique_urls
    
    def show_progress(self, page_num, max_pages, urls_found):
        """–ü–æ–∫–∞–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        progress = (page_num / max_pages) * 100
        print(f"\rüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {page_num}/{max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü ({progress:.0f}%) | "
              f"–¢–æ–≤–∞—Ä–æ–≤: {urls_found}", end='', flush=True)
    
    def _fetch_page(self, url):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=PARSER_CONFIG['timeout'])
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def _extract_product_urls(self, html, base_url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        urls = []
        
        if not html:
            return urls
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link['href']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–µ–¥–µ—Ç –ª–∏ –Ω–∞ —Ç–æ–≤–∞—Ä
            if self._is_product_url(href):
                full_url = self._make_absolute_url(href, base_url)
                if full_url and full_url not in urls:
                    urls.append(full_url)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∫–∞—Ä—Ç–æ—á–µ–∫
        product_cards = soup.find_all(['div', 'article'], class_=re.compile(r'card|product|item'))
        for card in product_cards:
            link = card.find('a', href=True)
            if link:
                href = link['href']
                if self._is_product_url(href):
                    full_url = self._make_absolute_url(href, base_url)
                    if full_url and full_url not in urls:
                        urls.append(full_url)
        
        return urls
    
    def _is_product_url(self, href):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Ç–æ–≤–∞—Ä–æ–º"""
        product_patterns = [
            r'\.html$',  # –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ .html
            r'/catalog/',  # —Å–æ–¥–µ—Ä–∂–∏—Ç /catalog/
            r'-\d+\.html$',  # —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—Ä—Ç–∏–∫—É–ª –≤ –∫–æ–Ω—Ü–µ
        ]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ —Ç–æ–≤–∞—Ä—ã
        exclude_patterns = [
            r'\.php$',
            r'\.xml$',
            r'\.json$',
            r'#',
            r'\?PAGEN_',
            r'/cart/',
            r'/auth/',
            r'/search/',
        ]
        
        href_lower = href.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        for pattern in exclude_patterns:
            if re.search(pattern, href_lower):
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
        for pattern in product_patterns:
            if re.search(pattern, href_lower):
                return True
        
        return False
    
    def _make_absolute_url(self, href, base_url):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            return urllib.parse.urljoin(self.base_url, href)
        else:
            return urllib.parse.urljoin(base_url, href)
    
    def _get_page_url(self, base_url, page_num):
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        if page_num == 1:
            return base_url
        else:
            # hello54.ru –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ?PAGEN_1=2 –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            separator = '?' if '?' not in base_url else '&'
            return f"{base_url}{separator}PAGEN_1={page_num}"
    
    def _has_next_page(self, html, current_page):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –∫–Ω–æ–ø–∫—É "–°–ª–µ–¥—É—é—â–∞—è" –∏–ª–∏ –Ω–æ–º–µ—Ä —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        next_patterns = [
            f'PAGEN_1={current_page + 1}',
            '–°–ª–µ–¥—É—é—â–∞—è',
            '–î–∞–ª–µ–µ',
            '>',
            '¬ª'
        ]
        
        for pattern in next_patterns:
            if soup.find('a', href=lambda href: href and pattern in str(href)):
                return True
        
        return False
    
    def _detect_total_pages(self, html):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        pagination = soup.find('div', class_=re.compile(r'pagination|pages'))
        if pagination:
            page_numbers = pagination.find_all('a', href=True)
            max_page = 1
            
            for link in page_numbers:
                href = link.get('href', '')
                match = re.search(r'PAGEN_1=(\d+)', href)
                if match:
                    page_num = int(match.group(1))
                    if page_num > max_page:
                        max_page = page_num
            
            return max_page
        
        return 1
    
    def _extract_category_name(self, url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ URL"""
        # –ü—Ä–∏–º–µ—Ä: https://hello54.ru/catalog/chekhly-dlya-smartfonov/
        match = re.search(r'/catalog/([^/]+)/', url)
        if match:
            name = match.group(1).replace('-', ' ').title()
            return name
        return None